{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1b1842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependencies\n",
    "\n",
    "import os\n",
    "import time \n",
    "import tqdm\n",
    "import io\n",
    "\n",
    "import numpy as np, pandas as pd\n",
    "import pickle\n",
    "import logging\n",
    "import random\n",
    "\n",
    "#import tensorflow as tf\n",
    "#from tensorflow.keras import layers\n",
    "\n",
    "from numpy import percentile, nan as np_nan\n",
    "\n",
    "import scipy\n",
    "from sklearn.metrics.pairwise import pairwise_kernels\n",
    "from gensim.utils import RepeatCorpusNTimes\n",
    "\n",
    "from PixelCorpora import PixelCorpus, PixelCorpusRW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91ea3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_ids = None\n",
    "ds_dir = 'slurm_job/mouse_brain/example_data/'\n",
    "#with open('ind_names.pickle', 'rb') as remaining_ions:\n",
    "#ind_name = pickle.load(remaining_ions)\n",
    "\n",
    "ind_name = None\n",
    "fdr = 0.1\n",
    "pix_per = 0.01\n",
    "i = 0.5\n",
    "window = 5\n",
    "q = 99.\n",
    "quan = 00.\n",
    "int_per = 0.5\n",
    "no_samples = 5\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2af8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_ns = 10\n",
    "SEED = 42\n",
    "window_size = 5\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "embedding_dim = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e934c2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_coloc_walk(coloc_matrix, n=5):\n",
    "    tmp = coloc_matrix.copy()\n",
    "    np.fill_diagonal(tmp, 0)\n",
    "    #transition_matrix = np.apply_along_axis(scipy.special.softmax, 0, tmp)\n",
    "    sequence = [np.random.choice(range(tmp.shape[0]))]\n",
    "    for i in range(n):\n",
    "        sequence.append(random.choices(range(tmp.shape[0]), weights=tmp[:, sequence[-1]])[0])\n",
    "    return sequence[1:]\n",
    "#random_coloc_walk(pairwise_kernels(ion_array, metric='cosine'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16c5a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for testing corpus building\n",
    "ion_ids = {}\n",
    "for f in os.listdir(ds_dir)[:1]:\n",
    "    try:\n",
    "        ds_df = pd.read_pickle(os.path.join(ds_dir,f))\n",
    "    except IsADirectoryError:\n",
    "        continue\n",
    "    all_ions = ds_df.drop(columns=['y','x']).columns.tolist()\n",
    "    logger.info(\"ds_df pixel size %i \", len(ds_df[['x', 'y']].drop_duplicates().index))\n",
    "\n",
    "    if ind_name != None:                                         \n",
    "        ion_names = list(set(ind_name).intersection(all_ions)) # intersection between all ions in the ds and specified ions\n",
    "        pop_ions = list(set(all_ions).difference(set(ion_names)))\n",
    "        ds_df = ds_df.drop(columns=pop_ions) # drop ions not in ind_name\n",
    "    else: ion_names = all_ions # either the specified ion names or all ions\n",
    "\n",
    "    if not ion_names:\n",
    "        pass # skip empty iterations\n",
    "    \n",
    "    for ion in ion_names:\n",
    "        ion_ids[ion] = ion_ids.get(ion, len(list(ion_ids.values())) +1 )\n",
    "    \n",
    "    # fiter out rows based on intensity and quantile param\n",
    "    int_thresh = percentile(ds_df, quan) * int_per  # this now uses a general intensity threshold,\n",
    "                                                   # could also use an ion specific one\n",
    "    filt_df = ds_df.rename(columns=ion_ids)\n",
    "    filt_df = filt_df.loc[~(filt_df.drop(columns=['y', 'x'])==0).all(axis=1)] # drop all zero rows\n",
    "    #filt_df[ion_names] = ds_df[ion_names][ds_df[ion_names] > int_thresh]\n",
    "    #filt_df = filt_df.astype(pd.SparseDtype(\"float\", np_nan))\n",
    "\n",
    "    # sample pixels\n",
    "    if pix_per != 1.0:\n",
    "        sampled_coord_df = filt_df.dropna(how='all').drop_duplicates().sample(frac=pix_per)\n",
    "    else: \n",
    "        sampled_coord_df = filt_df.dropna(how='all').drop_duplicates()\n",
    "    #logging.info(\"%i pixels selected for %s\", len(sampled_coord_df.index), f)\n",
    "\n",
    "    for _, c_row in sampled_coord_df.iloc[::2].iterrows(): #iloc[::2] takes every second row, inducing stride length of 2\n",
    "        x = c_row['x']  # center x coordinate\n",
    "        y = c_row['y']  # center y coordinate\n",
    "        # find rows corresponding to pixels around the sampled pixel coordinates\n",
    "        window_rows = filt_df[(filt_df['x'].between(x - w, x + w, inclusive = 'both')) \n",
    "                             & (filt_df['y'].between(y - w, y + w, inclusive = 'both'))]\n",
    "        \n",
    "        ion_rows = window_rows.drop(columns=['y','x'])\n",
    "        ion_rows = ion_rows.loc[:, (ion_rows != 0).any(axis=0)] # drop columns with all zero entries\n",
    "        coloc_matrix = pairwise_kernels(ion_rows.T, metric='cosine') # Don't forget the Transpose\n",
    "        for i in range(no_samples):\n",
    "            ions_idx = random_coloc_walk(coloc_matrix, n = 10)\n",
    "            ion_rows.columns[ions_idx].tolist()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05310b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "ion_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71a0c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for testing corpus building\n",
    "def vanilla_gen(shuff=1):\n",
    "    ion_ids = {}\n",
    "    for f in os.listdir(ds_dir)[:1]:\n",
    "        try:\n",
    "            ds_df = pd.read_pickle(os.path.join(ds_dir,f))\n",
    "        except IsADirectoryError:\n",
    "            continue\n",
    "        all_ions = ds_df.drop(columns=['y','x']).columns.tolist()\n",
    "        logger.info(\"ds_df pixel size %i \", len(ds_df[['x', 'y']].drop_duplicates().index))\n",
    "\n",
    "        if ind_name != None:                                         \n",
    "            ion_names = list(set(ind_name).intersection(all_ions)) # intersection between all ions in the ds and specified ions\n",
    "            pop_ions = list(set(all_ions).difference(set(ion_names)))\n",
    "            ds_df = ds_df.drop(columns=pop_ions) # drop ions not in ind_name\n",
    "        else: ion_names = all_ions # either the specified ion names or all ions\n",
    "\n",
    "        if not ion_names:\n",
    "            pass # skip empty iterations\n",
    "\n",
    "        for ion in ion_names:\n",
    "            ion_ids[ion] = ion_ids.get(ion, len(list(ion_ids.values())) +1 )\n",
    "\n",
    "        # fiter out rows based on intensity and quantile param\n",
    "        int_thresh = percentile(ds_df, quan) * int_per  # this now uses a general intensity threshold,\n",
    "                                                       # could also use an ion specific one\n",
    "        filt_df = ds_df.rename(columns=ion_ids)\n",
    "        filt_df = filt_df.loc[~(filt_df.drop(columns=['y', 'x'])==0).all(axis=1)] # drop all zero rows\n",
    "        #filt_df[ion_names] = ds_df[ion_names][ds_df[ion_names] > int_thresh]\n",
    "        #filt_df = filt_df.astype(pd.SparseDtype(\"float\", np_nan))\n",
    "\n",
    "        # sample pixels\n",
    "        if pix_per != 1.0:\n",
    "            sampled_coord_df = filt_df.dropna(how='all').drop_duplicates().sample(frac=pix_per)\n",
    "        else: \n",
    "            sampled_coord_df = filt_df.dropna(how='all').drop_duplicates()\n",
    "        #logging.info(\"%i pixels selected for %s\", len(sampled_coord_df.index), f)\n",
    "\n",
    "        for _, c_row in sampled_coord_df.iloc[::2].iterrows(): #iloc[::2] takes every second row, inducing stride length of 2\n",
    "            x = c_row['x']  # center x coordinate\n",
    "            y = c_row['y']  # center y coordinate\n",
    "            # find rows corresponding to pixels around the sampled pixel coordinates\n",
    "            window_rows = filt_df[(filt_df['x'].between(x - w, x + w, inclusive = 'both')) \n",
    "                                 & (filt_df['y'].between(y - w, y + w, inclusive = 'both'))]\n",
    "\n",
    "            exp_inds = []\n",
    "            ind_counts = dict(window_rows.drop(columns=['y', 'x']).count())\n",
    "            for ind in ind_counts:\n",
    "                for i in range(0, ind_counts[ind]): exp_inds.append(ind)\n",
    "            if shuff == 1:\n",
    "                random.shuffle(exp_inds) # shuffle ions in window\n",
    "            #random.shuffle(exp_inds) # shuffle ions in window\n",
    "            yield exp_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716c6774",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelCorpus(object):\n",
    "    def __init__(self, fdr_thresh=0.1, pix_per=0.5, int_per=0.5, window=5, quan=99., ind_name=None, ds_dir=None, ds_ids=None, stride = 1, shuffle = 1):\n",
    "        self.fdr = fdr_thresh\n",
    "        self.ds_dir = ds_dir\n",
    "        self.p = pix_per\n",
    "        self.i = int_per\n",
    "        self.w = window\n",
    "        self.q = quan\n",
    "        \n",
    "        self.ind_name = ind_name\n",
    "        self.ds_ids = ds_ids #new: list of metaspace dataset ids used as training data\n",
    "        self.stride = stride\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "    def __iter__(self):\n",
    "\n",
    "        for f in os.listdir(self.ds_dir):\n",
    "            try:\n",
    "                ds_df = pd.read_pickle(os.path.join(self.ds_dir,f))\n",
    "            except IsADirectoryError:\n",
    "                continue\n",
    "            all_ions = ds_df.drop(columns=['y','x']).columns.tolist()\n",
    "\n",
    "            if self.ind_name != None:                                         \n",
    "                ion_names = list(set(self.ind_name).intersection(all_ions)) # intersection between all ions in the ds and specified ions\n",
    "                pop_ions = list(set(all_ions).difference(set(ion_names)))\n",
    "                ds_df = ds_df.drop(columns=pop_ions) # drop ions not in ind_name\n",
    "                \n",
    "            else: ion_names = all_ions # either the specified ion names or all ions\n",
    "\n",
    "            if not ion_names:\n",
    "                pass # skip empty iterations\n",
    "\n",
    "            # fiter out rows based on intensity and quantile param\n",
    "            int_thresh = percentile(ds_df, self.q) * self.i  # this now uses a general intensity threshold,\n",
    "                                                           # could also use an ion specific one\n",
    "            filt_df = ds_df\n",
    "            filt_df[ion_names] = ds_df[ion_names][ds_df[ion_names] > int_thresh]\n",
    "            filt_df = filt_df.astype(pd.SparseDtype(\"int\", np_nan)) # int should be replaced, when we care about the exact number of intensity\n",
    "\n",
    "            # sample pixels\n",
    "            if self.p != 1.0:\n",
    "                sampled_coord_df = filt_df.dropna(how='all').drop_duplicates().sample(frac=self.p)\n",
    "            else: \n",
    "                sampled_coord_df = filt_df.dropna(how='all').drop_duplicates()\n",
    "            #logging.info(\"%i pixels selected for %s\", len(sampled_coord_df.index), f)\n",
    "\n",
    "            for _, c_row in sampled_coord_df.iloc[::self.stride].iterrows():\n",
    "                x = c_row['x']\n",
    "                y = c_row['y']\n",
    "\n",
    "                # find rows corresponding to pixels around the sampled pixel coordinates\n",
    "                window_rows = filt_df[(filt_df['x'].between(x - self.w, x + self.w, inclusive = 'both')) \n",
    "                                     & (filt_df['y'].between(y - self.w, y + self.w, inclusive = 'both'))]\n",
    "                # depending on how many items an ion/or formula (depending on self.ind_name parameter)\n",
    "                # occurs in the window, yield it\n",
    "                exp_inds = []\n",
    "                ind_counts = dict(window_rows[ion_names].count())\n",
    "                for ind in ind_counts:\n",
    "                    for i in range(0, ind_counts[ind]): exp_inds.append(ind)\n",
    "                if self.shuffle == 1:\n",
    "                    random.shuffle(exp_inds) # shuffle ions in window\n",
    "                yield exp_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f000e21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = PixelCorpus(fdr_thresh=0.1, pix_per=0.1, int_per=0, window=1, quan=50, ind_name=None, ds_dir='slurm_job/mouse_brain/example_data/', ds_ids=None, stride = 1, shuffle = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92696257",
   "metadata": {},
   "outputs": [],
   "source": [
    "from word2vec_pix import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1f9ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_ids = None; train = 'slurm_job/mouse_brain/example_data/'; ind_name = None; size = 100; threads = 6; min_count = 5; window = 1; sample = 1e-3; \n",
    "skipgram = 0; hs = 0; negative = 5; cbow_mean = 1; epochs = 1; fdr = 0.1; int_per = 0; pix_per = 0.01; quan = 50\n",
    "model = Word2Vec(\n",
    "        corpus=corpus,ds_ids = ds_ids, train=train, dict_size=None, ind_name= ind_name, \n",
    "        size=size, min_count=min_count, workers=threads,\n",
    "        window=window, sample=sample, sg=skipgram, hs=hs,\n",
    "        negative=negative, cbow_mean=1, iter=epochs, fdr=fdr, \n",
    "        int_per=int_per, pix_per=pix_per, quan = quan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ad46b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cbow_pair(model, word, input_word_indices, l1, alpha, learn_vectors=True, learn_hidden=True, compute_loss=False):\n",
    "    neu1e = zeros(l1.shape)\n",
    "\n",
    "    if model.hs:\n",
    "        l2a = model.syn1[word.point]  # 2d matrix, codelen x layer1_size\n",
    "        prod_term = dot(l1, l2a.T)\n",
    "        fa = expit(prod_term)  # propagate hidden -> output\n",
    "        ga = (1. - word.code - fa) * alpha  # vector of error gradients multiplied by the learning rate\n",
    "        if learn_hidden:\n",
    "            model.syn1[word.point] += outer(ga, l1)  # learn hidden -> output\n",
    "        neu1e += dot(ga, l2a)  # save error\n",
    "\n",
    "        # loss component corresponding to hierarchical softmax\n",
    "        if compute_loss:\n",
    "            sgn = (-1.0)**word.code  # ch function, 0-> 1, 1 -> -1\n",
    "            model.running_training_loss += sum(-log(expit(-sgn * prod_term)))\n",
    "\n",
    "    if model.negative:\n",
    "        # use this word (label = 1) + `negative` other random words not from this sentence (label = 0)\n",
    "        word_indices = [word.index]\n",
    "        while len(word_indices) < model.negative + 1:\n",
    "            w = model.cum_table.searchsorted(model.random.randint(model.cum_table[-1]))\n",
    "            if w != word.index:\n",
    "                word_indices.append(w)\n",
    "        l2b = model.syn1neg[word_indices]  # 2d matrix, k+1 x layer1_size\n",
    "        prod_term = dot(l1, l2b.T)\n",
    "        fb = expit(prod_term)  # propagate hidden -> output\n",
    "        gb = (neg_labels - fb) * alpha  # vector of error gradients multiplied by the learning rate\n",
    "        if learn_hidden:\n",
    "            model.syn1neg[word_indices] += outer(gb, l1)  # learn hidden -> output\n",
    "        neu1e += dot(gb, l2b)  # save error\n",
    "\n",
    "        # loss component corresponding to negative sampling\n",
    "        if compute_loss:\n",
    "            model.running_training_loss -= sum(log(expit(-1 * prod_term[1:])))  # for the sampled words\n",
    "            model.running_training_loss -= log(expit(prod_term[0]))  # for the output word\n",
    "\n",
    "    if learn_vectors:\n",
    "        # learn input -> hidden, here for all words in the window separately\n",
    "        if not model.cbow_mean and input_word_indices:\n",
    "            print(input_word_indices)\n",
    "            neu1e /= len(input_word_indices)\n",
    "        for i in input_word_indices:\n",
    "            print('input_word_indices', input_word_indices, '\\n')\n",
    "            model.wv.syn0[i] += neu1e * model.syn0_lockf[i] #replaced syn0 by vectors\n",
    "            print('neu1e', neu1e, '\\n')\n",
    "    return neu1e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca49e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "if random.shuffle == 0:\n",
    "    print('som')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9157299",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "\n",
    "neg_labels = zeros(model.negative + 1)\n",
    "neg_labels[0] = 1.\n",
    "\n",
    "alpha=0.025; compute_loss = False; zeros = np.zeros; dot = np.dot; outer = np.outer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f561e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle\n",
    "result = 0\n",
    "word_id_lists = corpus\n",
    "for word_id_list in word_id_lists:\n",
    "    word_vocabs = [model.wv.vocab[str(w)] for w in word_id_list if str(w) in model.wv.vocab and\n",
    "                   model.wv.vocab[str(w)].sample_int > model.random.rand() * 2**32]\n",
    "\n",
    "    for pos, word in enumerate(word_vocabs):\n",
    "        word2_indices = [word2.index for pos2, word2 in enumerate(word_vocabs) if (word2 is not None and pos2 != pos)]\n",
    "        l1 = np.sum(model.wv.syn0[word2_indices], axis=0)  # 1 x vector_size #replaced attribute syn0 by vectors\n",
    "        if word2_indices and model.cbow_mean:\n",
    "            l1 /= len(word2_indices)\n",
    "        train_cbow_pair(model, word, word2_indices, l1, alpha, compute_loss=compute_loss)\n",
    "    result += len(word_vocabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0bb76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no shuffle\n",
    "result = 0\n",
    "word_id_lists = corpus\n",
    "for word_id_list in word_id_lists:\n",
    "    word_vocabs = [model.wv.vocab[str(w)] for w in word_id_list if str(w) in model.wv.vocab and\n",
    "                   model.wv.vocab[str(w)].sample_int > model.random.rand() * 2**32]\n",
    "\n",
    "    for pos, word in enumerate(word_vocabs):\n",
    "        word2_indices = [word2.index for pos2, word2 in enumerate(word_vocabs) if (word2 is not None and pos2 != pos)]\n",
    "        l1 = np.sum(model.wv.syn0[word2_indices], axis=0)  # 1 x vector_size #replaced attribute syn0 by vectors\n",
    "        if word2_indices and model.cbow_mean:\n",
    "            l1 /= len(word2_indices)\n",
    "        train_cbow_pair(model, word, word2_indices, l1, alpha, compute_loss=compute_loss)\n",
    "    result += len(word_vocabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d51306b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d7468f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ions2idx = corpus.get_ions2ids()\n",
    "\n",
    "\n",
    "#vocab_size = len(ions2idx) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d533bdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_data(sequences,\n",
    "                           vocab_size,\n",
    "                           window_size,\n",
    "                           num_ns,\n",
    "                           seed):\n",
    "    # Elements of each training example are appended to these lists.\n",
    "    targets, contexts, labels = [], [], []\n",
    "\n",
    "    # Build the sampling table for `vocab_size` tokens.\n",
    "    sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n",
    "\n",
    "    # Iterate over all sequences (sentences) in the dataset.\n",
    "    for sequence in tqdm.tqdm(sequences):\n",
    "\n",
    "    # Generate positive skip-gram pairs for a sequence (sentence).\n",
    "        positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "          sequence,\n",
    "          vocabulary_size=vocab_size,\n",
    "          sampling_table=sampling_table,\n",
    "          window_size=window_size,\n",
    "          negative_samples=0)\n",
    "\n",
    "    # Iterate over each positive skip-gram pair to produce training examples\n",
    "    # with a positive context word and negative samples.\n",
    "        for target_word, context_word in positive_skip_grams:\n",
    "            context_class = tf.expand_dims(\n",
    "                tf.constant([context_word], dtype=\"int64\"), 1)\n",
    "            negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "                true_classes=context_class,\n",
    "                num_true=1,\n",
    "                num_sampled=num_ns,\n",
    "                unique=True,\n",
    "                range_max=vocab_size,\n",
    "                seed=seed,\n",
    "                name=\"negative_sampling\")\n",
    "\n",
    "            # Build context and label vectors (for one target word)\n",
    "            negative_sampling_candidates = tf.expand_dims(\n",
    "            negative_sampling_candidates, 1)\n",
    "\n",
    "            context = tf.concat([context_class, negative_sampling_candidates], 0)\n",
    "            label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
    "\n",
    "            # Append each element from the training example to global lists.\n",
    "            targets.append(target_word)\n",
    "            contexts.append(context)\n",
    "            labels.append(label)\n",
    "\n",
    "    return targets, contexts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391c8b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets, contexts, labels = generate_training_data(corpus,\n",
    "                            window_size= window_size,\n",
    "                            num_ns=num_ns,\n",
    "                            vocab_size=vocab_size,\n",
    "                            seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1946011d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e448a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = np.array(targets)\n",
    "contexts = np.array(contexts)[:,:,0]\n",
    "labels = np.array(labels)\n",
    "\n",
    "print('\\n')\n",
    "print(f\"targets.shape: {targets.shape}\")\n",
    "print(f\"contexts.shape: {contexts.shape}\")\n",
    "print(f\"labels.shape: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0f93cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1024\n",
    "BUFFER_SIZE = 10000\n",
    "dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset = dataset.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db2480d",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45d21a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(Word2Vec, self).__init__()\n",
    "        self.target_embedding = layers.Embedding(vocab_size,\n",
    "                                          embedding_dim,\n",
    "                                          input_length=1,\n",
    "                                          name=\"w2v_embedding\")\n",
    "        self.context_embedding = layers.Embedding(vocab_size,\n",
    "                                           embedding_dim,\n",
    "                                           input_length=num_ns+1)\n",
    "\n",
    "    def call(self, pair):\n",
    "        target, context = pair\n",
    "        # target: (batch, dummy?)  # The dummy axis doesn't exist in TF2.7+\n",
    "        # context: (batch, context)\n",
    "        if len(target.shape) == 2:\n",
    "            target = tf.squeeze(target, axis=1)\n",
    "        # target: (batch,)\n",
    "        word_emb = self.target_embedding(target)\n",
    "        # word_emb: (batch, embed)\n",
    "        context_emb = self.context_embedding(context)\n",
    "        # context_emb: (batch, context, embed)\n",
    "        dots = tf.einsum('be,bce->bc', word_emb, context_emb) \n",
    "        # dots: (batch, context)\n",
    "        return dots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2960eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ions2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35cf5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "t0 = time.time()\n",
    "model = Word2Vec(vocab_size, 20)\n",
    "model.compile(optimizer='adam',\n",
    "                 loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                 metrics=['accuracy'], \n",
    "                run_eagerly=True)\n",
    "model.fit(dataset, epochs=20)\n",
    "t1 = time.time()\n",
    "total = t1 - t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e80e536",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.get_layer('w2v_embedding').get_weights()[0]\n",
    "vocab = ions2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d417691",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, word in enumerate(vocab):\n",
    "    if index == 0:\n",
    "        continue  # skip 0, it's padding.\n",
    "    vec = weights[index]\n",
    "    print('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "    print(word + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd947e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.get_layer('w2v_embedding').get_weights()[0]\n",
    "vocab = ions2idx\n",
    "\n",
    "out_v = io.open('vectors_rw.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('metadata_rw.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for index, word in enumerate(vocab):\n",
    "    if index == 0:\n",
    "        continue  # skip 0, it's padding.\n",
    "    vec = weights[index]\n",
    "    out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "    out_m.write(word + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85c9162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now with theos sets\n",
    "ds_dir = 'slurm_job/theos_recom/No1'\n",
    "ind_name = None\n",
    "\n",
    "corpus = PixelCorpusRW(ds_ids = ds_ids, ds_dir = ds_dir, ind_name = ind_name, fdr_thresh = fdr,\n",
    "                     pix_per = pix_per, int_per = int_per, window = window, quan = quan, no_samples=5)\n",
    "\n",
    "repeater = RepeatCorpusNTimes(corpus, 1)\n",
    "\n",
    "ions2idx = corpus.get_ions2ids()\n",
    "vocab_size = len(ions2idx) + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7003b1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05153a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in corpus:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0b454c",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets, contexts, labels = generate_training_data(corpus,\n",
    "                            window_size= window_size,\n",
    "                            num_ns=num_ns,\n",
    "                            vocab_size=vocab_size,\n",
    "                            seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1809f3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = np.array(targets)\n",
    "contexts = np.array(contexts)[:,:,0]\n",
    "labels = np.array(labels)\n",
    "\n",
    "print('\\n')\n",
    "print(f\"targets.shape: {targets.shape}\")\n",
    "print(f\"contexts.shape: {contexts.shape}\")\n",
    "print(f\"labels.shape: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead3f421",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1024\n",
    "BUFFER_SIZE = 10000\n",
    "dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset = dataset.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e149f6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "model_th = Word2Vec(vocab_size, 20)\n",
    "model_th.compile(optimizer='adam',\n",
    "                 loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                 metrics=['accuracy'], \n",
    "                run_eagerly=True)\n",
    "model_th.fit(dataset, epochs=20)\n",
    "t1 = time.time()\n",
    "total = t1 - t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e4df34",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model_th.get_layer('w2v_embedding').get_weights()[0]\n",
    "vocab = ions2idx\n",
    "\n",
    "out_v = io.open('vectors_th_No1_rw.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('metadata_th_No1_rw.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for index, word in enumerate(vocab):\n",
    "    if index == 0:\n",
    "        continue  # skip 0, it's padding.\n",
    "    vec = weights[index]\n",
    "    out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "    out_m.write(word + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdeb423a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for testing corpus building\n",
    "\n",
    "for f in os.listdir(ds_dir):\n",
    "        try:\n",
    "            ds_df = pd.read_pickle(os.path.join(ds_dir,f))\n",
    "        except IsADirectoryError:\n",
    "            continue\n",
    "        all_ions = ds_df.drop(columns=['y','x']).columns.tolist()\n",
    "        logger.info(\"ds_df pixel size %i \", len(ds_df[['x', 'y']].drop_duplicates().index))\n",
    "\n",
    "        if ind_name != None:                                         \n",
    "            ion_names = list(set(ind_name).intersection(all_ions)) # intersection between all ions in the ds and specified ions\n",
    "            pop_ions = list(set(all_ions).difference(set(ion_names)))\n",
    "            ds_df = ds_df.drop(columns=pop_ions) # drop ions not in ind_name\n",
    "        else: ion_names = all_ions # either the specified ion names or all ions\n",
    "\n",
    "        if not ion_names:\n",
    "            pass # skip empty iterations\n",
    "\n",
    "        # fiter out rows based on intensity and quantile param\n",
    "        int_thresh = percentile(ds_df, quan) * int_per  # this now uses a general intensity threshold,\n",
    "                                                       # could also use an ion specific one\n",
    "        filt_df = ds_df\n",
    "        filt_df[ion_names] = ds_df[ion_names][ds_df[ion_names] > int_thresh]\n",
    "        filt_df = filt_df.astype(pd.SparseDtype(\"float\", np_nan))\n",
    "\n",
    "        # sample pixels\n",
    "        if p != 1.0:\n",
    "            sampled_coord_df = filt_df.dropna(how='all').drop_duplicates().sample(frac=p)\n",
    "        else: \n",
    "            sampled_coord_df = filt_df.dropna(how='all').drop_duplicates()\n",
    "        #logging.info(\"%i pixels selected for %s\", len(sampled_coord_df.index), f)\n",
    "\n",
    "        for _, c_row in sampled_coord_df.iterrows():\n",
    "            x = c_row['x']\n",
    "            y = c_row['y']\n",
    "\n",
    "            # find rows corresponding to pixels around the sampled pixel coordinates\n",
    "            window_rows = filt_df[(filt_df['x'].between(x - w, x + w, inclusive = 'both')) \n",
    "                                 & (filt_df['y'].between(y - w, y + w, inclusive = 'both'))]\n",
    "            # depending on how many items an ion/or formula (depending on self.ind_name parameter)\n",
    "            # occurs in the window, yield it\n",
    "            exp_inds = []\n",
    "            ind_counts = dict(window_rows[ion_names].count()) # count() number of column entries\n",
    "            for ind in ind_counts:\n",
    "                for i in range(0, ind_counts[ind]): exp_inds.append(ind)\n",
    "            # yield exp_inds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cf1392",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "exp_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d55990e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tqdm\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b630a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "big_list = list(chain.from_iterable(corpus_list))\n",
    "\n",
    "token_set = list(set(big_list))\n",
    "\n",
    "vocab, index = {}, 1 # start indexing from 1\n",
    "vocab['<pad>'] = 0 # add a padding token\n",
    "for token in token_set:\n",
    "    if token not in vocab:\n",
    "        vocab[token] = index\n",
    "        index += 1\n",
    "        \n",
    "vocab_size = len(vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93759c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fcbb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_vocab = {index: token for token, index in vocab.items()}\n",
    "print(inverse_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7b8695",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_tokens = big_list\n",
    "\n",
    "example_sequence = [vocab[word] for word in example_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afc8bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 2\n",
    "positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "      example_sequence,\n",
    "      vocabulary_size=vocab_size,\n",
    "      window_size=window_size,\n",
    "      negative_samples=0)\n",
    "print(len(positive_skip_grams))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b734334",
   "metadata": {},
   "outputs": [],
   "source": [
    "for target, context in positive_skip_grams[:5]:\n",
    "    print(f\"({target}, {context}): ({inverse_vocab[target]}, {inverse_vocab[context]})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f54d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of negative samples per positive context.\n",
    "num_ns = 4\n",
    "SEED = 42\n",
    "AUTOTUNE = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e949e6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get target and context words for one positive skip-gram.\n",
    "target_word, context_word = positive_skip_grams[0]\n",
    "\n",
    "\n",
    "\n",
    "context_class = tf.reshape(tf.constant(context_word, dtype=\"int64\"), (1, 1))\n",
    "negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "    true_classes=context_class,  # class that should be sampled as 'positive'\n",
    "    num_true=1,  # each positive skip-gram has 1 positive context class\n",
    "    num_sampled=num_ns,  # number of negative context words to sample\n",
    "    unique=True,  # all the negative samples should be unique\n",
    "    range_max=vocab_size,  # pick index of the samples from [0, vocab_size]\n",
    "    seed=SEED,  # seed for reproducibility\n",
    "    name=\"negative_sampling\"  # name of this operation\n",
    ")\n",
    "print(negative_sampling_candidates)\n",
    "print([inverse_vocab[index.numpy()] for index in negative_sampling_candidates])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08cf937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a dimension so you can use concatenation (in the next step).\n",
    "negative_sampling_candidates = tf.expand_dims(negative_sampling_candidates, 1)\n",
    "\n",
    "# Concatenate a positive context word with negative sampled words.\n",
    "context = tf.concat([context_class, negative_sampling_candidates], 0)\n",
    "\n",
    "# Label the first context word as `1` (positive) followed by `num_ns` `0`s (negative).\n",
    "label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
    "\n",
    "# Reshape the target to shape `(1,)` and context and label to `(num_ns+1,)`.\n",
    "target = tf.squeeze(target_word)\n",
    "context = tf.squeeze(context)\n",
    "label = tf.squeeze(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03995efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"target_index    : {target}\")\n",
    "print(f\"target_word     : {inverse_vocab[target_word]}\")\n",
    "print(f\"context_indices : {context}\")\n",
    "print(f\"context_words   : {[inverse_vocab[c.numpy()] for c in context]}\")\n",
    "print(f\"label           : {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac42a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"target  :\", target)\n",
    "print(\"context :\", context)\n",
    "print(\"label   :\", label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457d759b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assumes log_uniform distribution (Zipf)\n",
    "sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(size=10)\n",
    "print(sampling_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c51c211",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_data(sequences, window_size, num_ns, vocab_size, seed):\n",
    "    # Elements of each training example are appended to these lists.\n",
    "    targets, contexts, labels = [], [], []\n",
    "\n",
    "    # Build the sampling table for `vocab_size` tokens.\n",
    "    sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n",
    "\n",
    "    # Iterate over all sequences (sentences) in the dataset.\n",
    "    for sequence in tqdm.tqdm(sequences):\n",
    "\n",
    "    # Generate positive skip-gram pairs for a sequence (sentence).\n",
    "        positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "          sequence,\n",
    "          vocabulary_size=vocab_size,\n",
    "          sampling_table=sampling_table,\n",
    "          window_size=window_size,\n",
    "          negative_samples=0)\n",
    "\n",
    "    # Iterate over each positive skip-gram pair to produce training examples\n",
    "    # with a positive context word and negative samples.\n",
    "        for target_word, context_word in positive_skip_grams:\n",
    "            context_class = tf.expand_dims(\n",
    "                tf.constant([context_word], dtype=\"int64\"), 1)\n",
    "            negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "                true_classes=context_class,\n",
    "                num_true=1,\n",
    "                num_sampled=num_ns,\n",
    "                unique=True,\n",
    "                range_max=vocab_size,\n",
    "                seed=seed,\n",
    "                name=\"negative_sampling\")\n",
    "\n",
    "            # Build context and label vectors (for one target word)\n",
    "            negative_sampling_candidates = tf.expand_dims(\n",
    "            negative_sampling_candidates, 1)\n",
    "\n",
    "            context = tf.concat([context_class, negative_sampling_candidates], 0)\n",
    "            label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
    "\n",
    "            # Append each element from the training example to global lists.\n",
    "            targets.append(target_word)\n",
    "            contexts.append(context)\n",
    "            labels.append(label)\n",
    "\n",
    "    return targets, contexts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a515262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in case we have a new corpus\n",
    "with open('corpus_text_shuffled.txt', 'w') as f:\n",
    "    for window in corpus_list:\n",
    "        if window:         # write only, when not empty\n",
    "            for ion in window:\n",
    "                f.write(ion + ' ')\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abc6c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_ds = tf.data.TextLineDataset('corpus_text_shuffled.txt').filter(lambda x: tf.cast(tf.strings.length(x), bool))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0801d43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "vectorize_layer = layers.TextVectorization(\n",
    "    max_tokens = vocab_size, \n",
    "    output_mode='int', \n",
    "    standardize = None,\n",
    "    split = 'whitespace',\n",
    "    output_sequence_length=10\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ea0b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_layer.adapt(text_ds.batch(1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7023ae7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the created vocabulary for reference.\n",
    "inverse_vocab = vectorize_layer.get_vocabulary()\n",
    "print(inverse_vocab[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952cab6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the data in text_ds.\n",
    "text_vector_ds = text_ds.batch(1024).prefetch(AUTOTUNE).map(vectorize_layer).unbatch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4bdc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = list(text_vector_ds.as_numpy_iterator())\n",
    "print(len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0f07b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for seq in sequences[:5]:\n",
    "    print(f\"{seq} => {[inverse_vocab[i] for i in seq]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2966d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets, contexts, labels = generate_training_data(\n",
    "    sequences=sequences,\n",
    "    window_size=4,\n",
    "    num_ns=10,\n",
    "    vocab_size=vocab_size,\n",
    "    seed=SEED)\n",
    "\n",
    "targets = np.array(targets)\n",
    "contexts = np.array(contexts)[:,:,0]\n",
    "labels = np.array(labels)\n",
    "\n",
    "print('\\n')\n",
    "print(f\"targets.shape: {targets.shape}\")\n",
    "print(f\"contexts.shape: {contexts.shape}\")\n",
    "print(f\"labels.shape: {labels.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588adf9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1024\n",
    "BUFFER_SIZE = 100000\n",
    "dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cb9212",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5f19af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(Word2Vec, self).__init__()\n",
    "        self.target_embedding = layers.Embedding(vocab_size,\n",
    "                                          embedding_dim,\n",
    "                                          input_length=1,\n",
    "                                          name=\"w2v_embedding\")\n",
    "        self.context_embedding = layers.Embedding(vocab_size,\n",
    "                                           embedding_dim,\n",
    "                                           input_length=num_ns+1)\n",
    "\n",
    "    def call(self, pair):\n",
    "        target, context = pair\n",
    "        # target: (batch, dummy?)  # The dummy axis doesn't exist in TF2.7+\n",
    "        # context: (batch, context)\n",
    "        if len(target.shape) == 2:\n",
    "            target = tf.squeeze(target, axis=1)\n",
    "        # target: (batch,)\n",
    "        word_emb = self.target_embedding(target)\n",
    "        # word_emb: (batch, embed)\n",
    "        context_emb = self.context_embedding(context)\n",
    "        # context_emb: (batch, context, embed)\n",
    "        dots = tf.einsum('be,bce->bc', word_emb, context_emb) \n",
    "        # dots: (batch, context)\n",
    "        return dots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5d9221",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(x_logit, y_true):\n",
    "      return tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=y_true)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48dc1f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 20\n",
    "word2vec = Word2Vec(vocab_size, embedding_dim)\n",
    "word2vec.compile(optimizer='adam',\n",
    "                 loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                 metrics=['accuracy'], \n",
    "                run_eagerly=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74609350",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f458eed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a41c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec.fit(dataset, epochs=20, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993aa17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = word2vec.get_layer('w2v_embedding').get_weights()[0]\n",
    "vocab = vectorize_layer.get_vocabulary()\n",
    "\n",
    "out_v = io.open('vectors_norm_shuffled.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('metadata_norm_shuffled.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for index, word in enumerate(vocab):\n",
    "    if index == 0:\n",
    "        continue  # skip 0, it's padding.\n",
    "    vec = weights[index]\n",
    "    out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "    out_m.write(word + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554415f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import files\n",
    "    files.download('vectors_norm_shuffled.tsv')\n",
    "    files.download('metadata_norm_shuffled.tsv')\n",
    "except Exception:\n",
    "    pass\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (OldEnv)",
   "language": "python",
   "name": "oldenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
