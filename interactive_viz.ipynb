{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/geng/my-conda-envs/DomEnv/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n","2022-12-13 13:42:29.291676: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"]}],"source":["# imports\n","import os\n","import pickle\n","\n","import numpy as np, pandas as pd\n","import scipy\n","from sklearn.metrics import pairwise_kernels\n","import matplotlib.pyplot as plt\n","import seaborn as sns \n","\n","import metaspace\n","from metaspace import SMInstance\n","import re\n","\n","from sklearn.preprocessing import MultiLabelBinarizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","from viz import get_ds_list, get_ion_imgs, ion_cluster, plot_ion_imgs, label_point, imshow_ions, cluster_viz\n","from metadata import get_meta_df, post_processing\n","\n"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["# testing post_processing\n","vec_file = 'RW_output/vectors_TF_RW_validation_w1moreions_size100_ww5_rw.tsv'\n","name_file ='RW_output/metadata_TF_RW_validation_w1moreions_size100_ww5_rw.tsv' \n","test = post_processing(vec_file, name_file, u_embed=True)\n","meta_df = test.get_info_df()"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["# defaults, constants and helper functions\n","plt.rcParams['figure.figsize'] = (13,9)\n","plt.rcParams['figure.dpi'] = 300\n","\n","ds_ids = ['2017-08-03_15h09m06s', '2017-08-11_07h59m58s', '2017-08-03_15h09m51s']\n","ds_names = [\n","    'Servier_Ctrl_mouse_wb_median_plane_chca',\n","    'Servier_Ctrl_mouse_wb_lateral_plane_DHB',\n","    'Servier_Ctrl_mouse_wb_lateral_plane_chca']\n","ds_id2name = dict(zip(ds_ids, ds_names))\n","\n","color_dict = {'low' : 'red', 'high' : 'green', 'query' : 'black', 'none':'grey', 'offsample':'grey',\n","(1,0,0): 'red', (0,1,0): 'green', (0,0,1): 'blue',\n","(1,1,0): 'yellow', (1,0,1): 'purple', (1,1,1): 'black',\n","(0,1,1): 'cyan' }\n","\n","# Label OffSample\n","off_sample = []\n","sm = SMInstance()\n","for ds_id in ds_ids:\n","    ds = sm.dataset(id=ds_id)\n","    results = ds.results(database=(\"HMDB\", \"v4\"))\n","    tmp = results[results.offSample].ion.tolist()\n","    off_sample += tmp\n","\n","# Loading coloc ions\n","with open(\"triple_ions.pickle\", \"rb\") as trip: # ions annotated in all three datasets\n","    triple_ions = pickle.load(trip)\n","with open(\"top_ions.pickle\", \"rb\") as top: # ions with high coloc w.r.t. triple_ions\n","    top_ions = pickle.load(top)\n","with open(\"bot_ions.pickle\", \"rb\") as bot: # ions with low coloc w.r.t. triple_ions\n","    bot_ions = pickle.load(bot)\n","with open(\"train_ions.pickle\", \"rb\") as query:\n","    query_ions = pickle.load(query)\n","\n","# load the train_df (coloc of all ions in the 3 datasets with each other)\n","train_df = pd.read_csv('train_ions.csv', index_col=0)\n","# create an average coloc df\n","by_row_index = train_df.groupby(train_df.index)\n","mean_coloc_df = by_row_index.mean() # building the mean coloc spanning the 3 datasets\n","mean_coloc_df = mean_coloc_df[mean_coloc_df.index]\n"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["# Helper functions\n","def get_coloc_dict(df):\n","    ions = df['ion']\n","    d = {}\n","    for ion in ions:\n","        if ion in off_sample:\n","            d[ion] = 'offsample'\n","        elif ion in triple_ions:\n","            d[ion] = 'query'\n","        elif ion in top_ions:\n","            d[ion] = 'high'\n","        elif ion in bot_ions:\n","            d[ion] = 'low'\n","        else:\n","            d[ion] = 'none'\n","    return d\n","\n","def preprocess_meta_df(df, ds_ids = ds_ids):\n","    true_ids = [list(set(\n","        df_ids).intersection(set(ds_ids)))\n","        for df_ids in df['dataset_ids'].tolist()\n","        ]\n","    true_names = [list(set(\n","        df_names).intersection(set(ds_names)))\n","        for df_names in df['dataset_names'].tolist()\n","        ]\n","    df['dataset_ids'] = true_ids\n","    df['dataset_names'] = true_names\n","\n","    # Multihot encoding datasets\n","    df.drop(columns=['encoded_ds'], inplace=True)\n","    multi_hot = MultiLabelBinarizer()\n","    encoded = multi_hot.fit_transform(df['dataset_names'])\n","    # turning lists into tuples\n","    encoded_tuples = [tuple(l) for l in list(encoded)]\n","    df['encoded'] = encoded_tuples\n","    ion2coloc=get_coloc_dict(df)\n","    df['coloc'] = df['ion'].map(ion2coloc)\n","    df.index = df['ion']\n","    return df\n","\n","\n","def noco_distances(mean_coloc_df, cos_df):\n","    '''\n","    params:\n","    --------\n","    mean_coloc_df : coloc_df averaged over train datasets\n","                    necessary for co-occurrence info\n","    cos_df        : cosine similarity df \n","\n","    returns:\n","    --------\n","    (no_dist, co_dist): a tuple of dictionaries, with entries for non and co-occurring ions\n","                        with individual entries (ion_1, ion_2) : cosine_sim(ion_1, ion_2)\n","    '''\n","    noco_ions = [(ion1, ion2)    \n","        for ion1 in mean_coloc_df.index for ion2 in mean_coloc_df.columns \n","        if mean_coloc_df.isna().loc[ion1, ion2]]\n","\n","    co_ions = [(ion1, ion2)    \n","        for ion1 in mean_coloc_df.index for ion2 in mean_coloc_df.columns \n","        if not mean_coloc_df.isna().loc[ion1, ion2]] \n","\n","    # get cosine distance for not co-occurring ion pairs\n","    no_dist = {pair : cos_df.loc[pair[0],pair[1]]\n","        for pair in noco_ions}\n","    # get cosine distance for co-occuring ion pairs\n","    co_dist = {pair : cos_df.loc[pair[0],pair[1]]\n","        for pair in co_ions if pair[0] != pair[1]} # drop self similarity\n","\n","    return no_dist, co_dist\n","\n","def cos_coloc_dict(mean_coloc_df, cos_df):\n","    '''\n","    returns a dictionary with ion pair keys and (cos_sim, coloc) values\n","    '''\n","    cos_df = cos_df.loc[mean_coloc_df.index, mean_coloc_df.columns]\n","    cos_coloc_dict = {\n","        (ion1, ion2): (cos_df.loc[ion1,ion2], mean_coloc_df.loc[ion1, ion2]) \n","        for ion1 in cos_df.index for ion2 in mean_coloc_df.index}\n","    return cos_coloc_dict\n","\n","def pre_process(model, vec_file, meta_file = None, embed=True):\n","    if model == 'rw':\n","        df = get_meta_df(vec_file, meta_file, embed = embed)\n","        # process i2v embeddings\n","        iv = pd.read_csv(vec_file, sep = '\\t', header= None)   # read ion vectors and their labels\n","        vec_names = pd.read_csv(meta_file, sep = '\\t', header = None)\n","        idx2ion = vec_names.to_dict()[0]\n","        iv.index = iv.index.map(idx2ion)\n","\n","        cos_df = pd.DataFrame(cosine_similarity(iv))\n","        cos_df.index = cos_df.index.map(idx2ion)\n","        cos_df.columns = cos_df.columns.map(idx2ion)\n","\n","    elif model == 'vanilla':\n","        df = get_meta_df(vec_file, txt=True, embed = embed)\n","        # processing ionvecs\n","        with open(vec_file, 'r') as f:\n","            lines = [i.split() for i in f]\n","        ion2vec = {line[0]: list(map(float, line[1:])) for line in lines[1:]}\n","\n","        iv = pd.DataFrame(ion2vec).T\n","        cos_df = pd.DataFrame(cosine_similarity(iv), columns=iv.index, index=iv.index)\n","\n","    df = preprocess_meta_df(df)\n","    df = df.loc[mean_coloc_df.index, :]\n","    \n","    cos_df = cos_df.loc[mean_coloc_df.index, mean_coloc_df.columns]\n","\n","    coord_dict = {\n","        (ion1, ion2): (cos_df.loc[ion1,ion2], mean_coloc_df.loc[ion1, ion2]) \n","        for ion1 in cos_df.index for ion2 in mean_coloc_df.index}\n","\n","    no_dist, co_dist = nocoloc_distances(mean_coloc_df, cos_df)\n","\n","    return df, coord_dict, (no_dist, co_dist)\n"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|████████████████████████████████████████| 34/34 [00:00<00:00, 120.80it/s]\n","100%|████████████████████████████████████████| 39/39 [00:00<00:00, 121.48it/s]\n","100%|████████████████████████████████████████| 40/40 [00:00<00:00, 128.32it/s]\n","100%|████████████████████████████████████████| 34/34 [00:00<00:00, 138.69it/s]\n","100%|████████████████████████████████████████| 39/39 [00:00<00:00, 181.17it/s]\n","100%|████████████████████████████████████████| 40/40 [00:00<00:00, 142.96it/s]\n","100%|████████████████████████████████████████| 34/34 [00:00<00:00, 91.96it/s] \n","100%|████████████████████████████████████████| 39/39 [00:00<00:00, 161.39it/s]\n","100%|████████████████████████████████████████| 40/40 [00:00<00:00, 164.60it/s]\n","100%|████████████████████████████████████████| 34/34 [00:00<00:00, 140.42it/s]\n","100%|████████████████████████████████████████| 39/39 [00:00<00:00, 181.26it/s]\n","100%|████████████████████████████████████████| 40/40 [00:00<00:00, 159.90it/s]\n"]}],"source":["# Preprocess gensim vanilla data trained on all ions, with embed size 50\n","vanilla_all50_dfs = {}\n","vanilla_all50_coords = {}\n","vanilla_all50_dists = {}\n","\n","for i in range(1,6):\n","    try:\n","        gen_file = f\"slurm_job/Vanilla_output/gensim_validation_w{i}_noshuff30size50moreions.model.txt\"\n","        p = post_processing(gen_file, u_embed=True, dsid_list=ds_ids)\n","        df = p.get_info_df()\n","        col_df = p.get_mean_coloc(ds_ids, query_ions)\n","        cos_df = p.get_embed_sim(query_ions)\n","        \n","        ion2coloc=get_coloc_dict(df) # add coloc info\n","        df['coloc'] = df['ion'].map(ion2coloc) \n","\n","        vanilla_all50_dfs[f'w{i}'] = df\n","        vanilla_all50_coords[f'w{i}'] = cos_coloc_dict(col_df, cos_df)\n","        vanilla_all50_dists[f'w{i}'] = noco_distances(col_df, cos_df)\n","    except FileNotFoundError: continue\n","\n"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|████████████████████████████████████████| 34/34 [00:00<00:00, 124.85it/s]\n","100%|████████████████████████████████████████| 39/39 [00:00<00:00, 156.28it/s]\n","100%|████████████████████████████████████████| 40/40 [00:00<00:00, 176.55it/s]\n","100%|████████████████████████████████████████| 34/34 [00:00<00:00, 154.17it/s]\n","100%|████████████████████████████████████████| 39/39 [00:01<00:00, 33.04it/s]\n","100%|████████████████████████████████████████| 40/40 [00:00<00:00, 152.96it/s]\n","100%|████████████████████████████████████████| 34/34 [00:00<00:00, 120.42it/s]\n","100%|████████████████████████████████████████| 39/39 [00:00<00:00, 167.49it/s]\n","100%|████████████████████████████████████████| 40/40 [00:00<00:00, 177.07it/s]\n","100%|████████████████████████████████████████| 34/34 [00:00<00:00, 145.95it/s]\n","100%|████████████████████████████████████████| 39/39 [00:00<00:00, 176.56it/s]\n","100%|████████████████████████████████████████| 40/40 [00:00<00:00, 151.86it/s]\n"]}],"source":["# Preprocess gensim vanilla data trained on all ions, with embed size 100\n","vanilla_all100_dfs = {}\n","vanilla_all100_coords = {}\n","vanilla_all100_dists = {}\n","\n","for i in range(1,6):\n","    try:\n","        gen_file = f\"slurm_job/Vanilla_output/gensim_validation_w{i}_noshuff30size100moreions.model.txt\"\n","        p = post_processing(gen_file, u_embed=True, dsid_list=ds_ids)\n","        df = p.get_info_df()\n","        col_df = p.get_mean_coloc(ds_ids, query_ions)\n","        cos_df = p.get_embed_sim(query_ions)\n","        \n","        ion2coloc=get_coloc_dict(df) # add coloc info\n","        df['coloc'] = df['ion'].map(ion2coloc) \n","\n","        vanilla_all50_dfs[f'w{i}'] = df\n","        vanilla_all50_coords[f'w{i}'] = cos_coloc_dict(col_df, cos_df)\n","        vanilla_all50_dists[f'w{i}'] = noco_distances(col_df, cos_df)\n","    except FileNotFoundError: continue\n"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|████████████████████████████████████████| 34/34 [00:00<00:00, 156.08it/s]\n","100%|████████████████████████████████████████| 39/39 [00:00<00:00, 201.17it/s]\n","100%|████████████████████████████████████████| 40/40 [00:00<00:00, 173.03it/s]\n","100%|████████████████████████████████████████| 34/34 [00:00<00:00, 140.11it/s]\n","100%|████████████████████████████████████████| 39/39 [00:00<00:00, 193.61it/s]\n","100%|████████████████████████████████████████| 40/40 [00:00<00:00, 190.01it/s]\n","100%|████████████████████████████████████████| 34/34 [00:00<00:00, 132.96it/s]\n","100%|████████████████████████████████████████| 39/39 [00:00<00:00, 183.27it/s]\n","100%|████████████████████████████████████████| 40/40 [00:00<00:00, 192.14it/s]\n","100%|████████████████████████████████████████| 34/34 [00:00<00:00, 151.05it/s]\n","100%|████████████████████████████████████████| 39/39 [00:00<00:00, 193.05it/s]\n","100%|████████████████████████████████████████| 40/40 [00:00<00:00, 179.24it/s]\n","100%|████████████████████████████████████████| 34/34 [00:00<00:00, 163.57it/s]\n","100%|████████████████████████████████████████| 39/39 [00:00<00:00, 184.55it/s]\n","100%|████████████████████████████████████████| 40/40 [00:00<00:00, 185.11it/s]\n"]}],"source":["# Preprocess gensim vanilla no_shuffling_20it only trained on query ions\n","vanilla_20_dfs = {}\n","vanilla_20_coords= {}\n","vanilla_20_dists = {}\n","\n","for i in range(1,6):\n","    try:\n","        gen_file = f\"slurm_job/Vanilla_output/gensim_validation_w{i}_noshuff.model.txt\"\n","        p = post_processing(gen_file, u_embed=True, dsid_list=ds_ids)\n","        df = p.get_info_df()\n","        # query_ions have changed slighlty from the train ions as we filtered out\n","        # off_sample ions such that we have to intersect between old and new train ions\n","        temp_ions = df['ion'].to_list()\n","        query_ions_prime = list(set(temp_ions).intersection(set(query_ions)))\n","        col_df = p.get_mean_coloc(ds_ids, query_ions_prime)\n","        cos_df = p.get_embed_sim(query_ions_prime)\n","        \n","        ion2coloc=get_coloc_dict(df) # add coloc info\n","        df['coloc'] = df['ion'].map(ion2coloc) \n","\n","        vanilla_20_dfs[f'w{i}'] = df\n","        vanilla_20_coords[f'w{i}'] = cos_coloc_dict(col_df, cos_df)\n","        vanilla_20_dists[f'w{i}'] = noco_distances(col_df, cos_df)\n","    except FileNotFoundError: continue\n"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|████████████████████████████████████████| 34/34 [00:00<00:00, 146.23it/s]\n","100%|████████████████████████████████████████| 39/39 [00:00<00:00, 177.00it/s]\n","100%|████████████████████████████████████████| 40/40 [00:00<00:00, 182.00it/s]\n","100%|████████████████████████████████████████| 34/34 [00:00<00:00, 141.24it/s]\n","100%|████████████████████████████████████████| 39/39 [00:00<00:00, 59.55it/s]\n","100%|████████████████████████████████████████| 40/40 [00:00<00:00, 178.42it/s]\n","100%|████████████████████████████████████████| 34/34 [00:00<00:00, 151.52it/s]\n","100%|████████████████████████████████████████| 39/39 [00:00<00:00, 177.07it/s]\n","100%|████████████████████████████████████████| 40/40 [00:00<00:00, 144.77it/s]\n","100%|████████████████████████████████████████| 34/34 [00:00<00:00, 145.20it/s]\n","100%|████████████████████████████████████████| 39/39 [00:00<00:00, 191.48it/s]\n","100%|████████████████████████████████████████| 40/40 [00:00<00:00, 157.96it/s]\n","100%|████████████████████████████████████████| 34/34 [00:00<00:00, 147.44it/s]\n","100%|████████████████████████████████████████| 39/39 [00:00<00:00, 165.00it/s]\n","100%|████████████████████████████████████████| 40/40 [00:00<00:00, 187.64it/s]\n"]}],"source":["# Preprocessing RW data, trained on query ions only\n","rw_dfs = {}\n","rw_coords = {}\n","rw_dists = {}\n","for i in range(1,6):\n","    try:\n","        vec_file = f\"RW_output/vectors_TF_RW_validation_w{i}_rw.tsv\"\n","        meta_file = f\"RW_output/metadata_TF_RW_validation_w{i}_rw.tsv\"\n","        p = post_processing(vec_file=vec_file, name_file=meta_file,\n","                             u_embed=True, dsid_list=ds_ids)\n","        df = p.get_info_df()\n","        # query_ions have changed slighlty from the train ions as we filtered out\n","        # off_sample ions such that we have to intersect between old and new train ions\n","        temp_ions = df['ion'].to_list()\n","        query_ions_prime = list(set(temp_ions).intersection(set(query_ions)))\n","        col_df = p.get_mean_coloc(ds_ids, query_ions_prime)\n","        cos_df = p.get_embed_sim(query_ions_prime)\n","        \n","        ion2coloc=get_coloc_dict(df) # add coloc info\n","        df['coloc'] = df['ion'].map(ion2coloc) \n","\n","        rw_dfs[f'w{i}'] = df\n","        rw_coords[f'w{i}'] = cos_coloc_dict(col_df, cos_df)\n","        rw_dists[f'w{i}'] = noco_distances(col_df, cos_df)\n","    except FileNotFoundError: continue"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|████████████████████████████████████████| 34/34 [00:00<00:00, 135.57it/s]\n","100%|████████████████████████████████████████| 39/39 [00:00<00:00, 186.74it/s]\n","100%|████████████████████████████████████████| 40/40 [00:00<00:00, 193.11it/s]\n","100%|████████████████████████████████████████| 34/34 [00:00<00:00, 145.04it/s]\n","100%|████████████████████████████████████████| 39/39 [00:00<00:00, 181.24it/s]\n","100%|████████████████████████████████████████| 40/40 [00:00<00:00, 162.72it/s]\n","100%|████████████████████████████████████████| 34/34 [00:00<00:00, 155.41it/s]\n","100%|████████████████████████████████████████| 39/39 [00:00<00:00, 180.21it/s]\n","100%|████████████████████████████████████████| 40/40 [00:00<00:00, 152.08it/s]\n","100%|████████████████████████████████████████| 34/34 [00:00<00:00, 139.98it/s]\n","100%|████████████████████████████████████████| 39/39 [00:00<00:00, 167.93it/s]\n","100%|████████████████████████████████████████| 40/40 [00:00<00:00, 177.80it/s]\n","100%|████████████████████████████████████████| 34/34 [00:00<00:00, 151.70it/s]\n","100%|████████████████████████████████████████| 39/39 [00:00<00:00, 193.54it/s]\n","100%|████████████████████████████████████████| 40/40 [00:00<00:00, 196.01it/s]\n"]}],"source":["# Preprocessing RW data where we trained on all ions, with ww10\n","rw_all10_dfs = {}\n","rw_all10_coords = {}\n","rw_all10_dists = {}\n","for i in range(1,6):\n","    try:\n","        vec_file = f\"RW_output/vectors_TF_RW_validation_w{i}moreions_rw.tsv\"\n","        meta_file = f\"RW_output/metadata_TF_RW_validation_w{i}moreions_rw.tsv\"\n","        p = post_processing(vec_file=vec_file, name_file=meta_file,\n","                             u_embed=True, dsid_list=ds_ids)\n","        df = p.get_info_df()\n","        col_df = p.get_mean_coloc(ds_ids, query_ions)\n","        cos_df = p.get_embed_sim(query_ions)\n","        \n","        ion2coloc=get_coloc_dict(df) # add coloc info\n","        df['coloc'] = df['ion'].map(ion2coloc) \n","\n","        rw_all10_dfs[f'w{i}'] = df\n","        rw_all10_coords[f'w{i}'] = cos_coloc_dict(col_df, cos_df)\n","        rw_all10_dists[f'w{i}'] = noco_distances(col_df, cos_df)\n","    except FileNotFoundError: continue"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|████████████████████████████████████████| 34/34 [00:00<00:00, 112.52it/s]\n","100%|████████████████████████████████████████| 39/39 [00:00<00:00, 174.53it/s]\n","100%|████████████████████████████████████████| 40/40 [00:00<00:00, 172.45it/s]\n","100%|████████████████████████████████████████| 34/34 [00:00<00:00, 160.75it/s]\n","100%|████████████████████████████████████████| 39/39 [00:00<00:00, 184.70it/s]\n","100%|████████████████████████████████████████| 40/40 [00:00<00:00, 177.12it/s]\n","100%|████████████████████████████████████████| 34/34 [00:00<00:00, 153.48it/s]\n","100%|████████████████████████████████████████| 39/39 [00:00<00:00, 183.95it/s]\n","100%|████████████████████████████████████████| 40/40 [00:00<00:00, 191.37it/s]\n","100%|████████████████████████████████████████| 34/34 [00:00<00:00, 161.78it/s]\n","100%|████████████████████████████████████████| 39/39 [00:00<00:00, 192.31it/s]\n","100%|████████████████████████████████████████| 40/40 [00:00<00:00, 162.57it/s]\n","100%|████████████████████████████████████████| 34/34 [00:00<00:00, 130.82it/s]\n","100%|████████████████████████████████████████| 39/39 [00:00<00:00, 136.23it/s]\n","100%|████████████████████████████████████████| 40/40 [00:00<00:00, 150.56it/s]\n"]}],"source":["# Preprocessing RW data where we trained on all ions with ww5\n","rw_all5_dfs = {}\n","rw_all5_coords = {}\n","rw_all5_dists = {}\n","for i in range(1,6):\n","    try:\n","        vec_file = f\"RW_output/vectors_TF_RW_validation_w{i}moreions_size50_ww5_rw.tsv\"\n","        meta_file = f\"RW_output/metadata_TF_RW_validation_w{i}moreions_size50_ww5_rw.tsv\"\n","\n","        p = post_processing(vec_file=vec_file, name_file=meta_file,\n","                             u_embed=True, dsid_list=ds_ids)\n","        df = p.get_info_df()\n","        col_df = p.get_mean_coloc(ds_ids, query_ions)\n","        cos_df = p.get_embed_sim(query_ions)\n","        \n","        ion2coloc=get_coloc_dict(df) # add coloc info\n","        df['coloc'] = df['ion'].map(ion2coloc) \n","\n","        rw_all5_dfs[f'w{i}'] = df\n","        rw_all5_coords[f'w{i}'] = cos_coloc_dict(col_df, cos_df)\n","        rw_all5_dists[f'w{i}'] = noco_distances(col_df, cos_df)\n","    except FileNotFoundError: continue "]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|████████████████████████████████████████| 34/34 [00:00<00:00, 145.25it/s]\n","100%|████████████████████████████████████████| 39/39 [00:00<00:00, 96.78it/s] \n","100%|████████████████████████████████████████| 40/40 [00:00<00:00, 170.72it/s]\n","100%|████████████████████████████████████████| 34/34 [00:00<00:00, 129.07it/s]\n","100%|████████████████████████████████████████| 39/39 [00:00<00:00, 169.21it/s]\n","100%|████████████████████████████████████████| 40/40 [00:00<00:00, 182.12it/s]\n","100%|████████████████████████████████████████| 34/34 [00:00<00:00, 152.11it/s]\n","100%|████████████████████████████████████████| 39/39 [00:00<00:00, 195.58it/s]\n","100%|████████████████████████████████████████| 40/40 [00:00<00:00, 147.16it/s]\n","100%|████████████████████████████████████████| 34/34 [00:00<00:00, 134.84it/s]\n","100%|████████████████████████████████████████| 39/39 [00:00<00:00, 191.35it/s]\n","100%|████████████████████████████████████████| 40/40 [00:00<00:00, 180.14it/s]\n","100%|████████████████████████████████████████| 34/34 [00:00<00:00, 114.65it/s]\n","100%|████████████████████████████████████████| 39/39 [00:00<00:00, 181.13it/s]\n","100%|████████████████████████████████████████| 40/40 [00:00<00:00, 181.98it/s]\n"]}],"source":["# Preprocessing RW data where we trained on all ions with ww5 and embed_size 100\n","rw_all5_100_dfs = {}\n","rw_all5_100_coords = {}\n","rw_all5_100_dists = {}\n","for i in range(1,6):\n","    try:\n","        vec_file = f\"RW_output/vectors_TF_RW_validation_w{i}moreions_size100_ww5_rw.tsv\"\n","        meta_file = f\"RW_output/metadata_TF_RW_validation_w{i}moreions_size100_ww5_rw.tsv\"\n","        p = post_processing(vec_file=vec_file, name_file=meta_file,\n","                             u_embed=True, dsid_list=ds_ids)\n","        df = p.get_info_df()\n","        col_df = p.get_mean_coloc(ds_ids, query_ions)\n","        cos_df = p.get_embed_sim(query_ions)\n","        \n","        ion2coloc=get_coloc_dict(df) # add coloc info\n","        df['coloc'] = df['ion'].map(ion2coloc) \n","\n","        rw_all5_100_dfs[f'w{i}'] = df\n","        rw_all5_100_coords[f'w{i}'] = cos_coloc_dict(col_df, cos_df)\n","        rw_all5_100_dists[f'w{i}'] = noco_distances(col_df, cos_df)\n","    except FileNotFoundError: continue "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Plotting vanilla vectors\n","for i, (df, coords, dists) in enumerate(\n","    zip(vanilla_all50_dfs.values(), vanilla_all50_coords.values(), vanilla_all50_dists.values()), 1):\n","    w = (2*i)+1 \n","    title = (f'Vanilla with 30 epochs and embedding dimension 50, window size ${w} \\\\times {w} $, trained all ions')\n","    save_str = f'plots/validation/vanilla/vanilla_30eps_50dims_w{i}_'\n","\n","    ax = sns.scatterplot(df, x = 'umap_x', y='umap_y', hue='coloc', palette=color_dict, s=40)\n","    ax.set_title(title + 'colored by coloc')\n","    label_point(df['umap_x'], df['umap_y'], df['mol_name'], ax, size=4)\n","    plt.savefig(save_str + 'umap_coloc.png')\n","    plt.show()\n","    \n","    ax = sns.scatterplot(df, x = 'umap_x', y='umap_y', hue='encoded', palette=color_dict, s=40)\n","    ax.set_title(title + 'colored by co-occurrence')\n","    label_point(df['umap_x'], df['umap_y'], df['mol_name'], ax, size=4)\n","    plt.savefig(save_str + 'umap_encoded.png', format='png')\n","    plt.show()\n","\n","    xy = np.array(list(coords.values()))\n","    ax = sns.scatterplot(x = xy[:,0], y= xy[:,1])\n","    ax.set_title(title + 'Mean coloc over cosine distance')\n","    ax.set_ylabel('Average Colocalization')\n","    ax.set_xlabel('Cosine distance in embedded space')\n","    plt.savefig(save_str + 'cos_over_coloc.png', format='png')\n","    plt.show()\n","\n","    no_dist, co_dist = dists\n","    ax = sns.violinplot([list(no_dist.values()), list(co_dist.values())], \n","    linewidth=2, fliersize=2)\n","    ax.set_ylabel('Cosine distance in embedded space')\n","    ax.set_title(title + 'Cosine distance w.r.t. co-occurrence')\n","    ax.set_xticklabels(labels=['no co-occurence', 'co-occurence'])\n","    plt.savefig(save_str + 'cos_violin.png', format='png')\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Plotting the random walk embeddings trained on all ions for ww5\n","for i, (df, coords, dists) in enumerate(\n","    zip(rw_dfs.values(), rw_coords.values(), rw_dists.values()), 1):\n","    w = (2*i)+1 \n","    title = f'RW, 30 epochs and 100 embedding dimensions, window size ${w} \\\\times {w} $, word window 10, query only'\n","    save_str = f'plots/validation/rw/rw_30eps_100dims_w{i}_ww10_queryonly' \n","\n","    ax = sns.scatterplot(df, x = 'umap_x', y='umap_y', hue='coloc', palette=color_dict, s=40)\n","    ax.set_title(title + 'colored by coloc')\n","    label_point(df['umap_x'], df['umap_y'], df['mol_name'], ax, size=4)\n","    plt.savefig(save_str + 'umap_coloc.png')\n","    plt.show()\n","    ax = sns.scatterplot(df, x = 'umap_x', y='umap_y', hue='encoded', palette=color_dict, s=40)\n","    ax.set_title(title + 'colored by co-occurrence')\n","    label_point(df['umap_x'], df['umap_y'], df['mol_name'], ax, size=4)\n","    plt.savefig(save_str + 'umap_encoded.png', format='png')\n","    plt.show()\n","\n","    xy = np.array(list(coords.values()))\n","    ax = sns.scatterplot(x = xy[:,0], y= xy[:,1])\n","    ax.set_title(title + 'Mean coloc over cosine distance')\n","    ax.set_ylabel('Average Colocalization')\n","    ax.set_xlabel('Cosine distance in embedded space')\n","    plt.savefig(save_str + 'cos_over_coloc.png', format='png')\n","    plt.show()\n","\n","    no_dist, co_dist = dists\n","    ax = sns.violinplot([list(no_dist.values()), list(co_dist.values())], \n","    linewidth=2, fliersize=2)\n","    ax.set_ylabel('Cosine distance in embedded space')\n","    ax.set_title(title + 'Cosine distance w.r.t. co-occurrence')\n","    ax.set_xticklabels(labels=['no co-occurence', 'co-occurence'])\n","    plt.savefig(save_str + 'cos_violin.png', format='png')\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"interpreter":{"hash":"c193994c00d86a66507bbd7c6d4c7b0217819164f6c9fb39290b4b0e58df0016"},"kernelspec":{"display_name":"Python 3.9.13 ('DomEnv': conda)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
