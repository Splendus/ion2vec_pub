{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6cf143f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-09 07:13:35.301297: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from PixelCorpora import PixelCorpus, PixelCorpusRW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "7ee49f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
      "Number of devices: 1\n"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
    "\n",
    "ds_ids = None\n",
    "train = \"slurm_job/theos_recom/No2_neg/\"\n",
    "fdr = 0.1\n",
    "window = 5\n",
    "int_per = 0.5\n",
    "quan = 0\n",
    "pix_per = 0.5\n",
    "ind_name = None\n",
    "stride = 4\n",
    "\n",
    "corpus = PixelCorpusRW(ds_ids = ds_ids, ds_dir = train, ind_name = ind_name, fdr_thresh = fdr,\n",
    "                     pix_per = pix_per, int_per = int_per, window = window, quan = quan, stride = stride)\n",
    "\n",
    "ions2idx = corpus.get_ions2ids()\n",
    "vocab_size = len(ions2idx) + 2\n",
    "\n",
    "num_ns = 10\n",
    "SEED = 42\n",
    "window_size = 5\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "embedding_dim = 20\n",
    "BATCH_SIZE = 1024\n",
    "BUFFER_SIZE = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fe6c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = chunks(corpus, batchsize=15)\n",
    "for batch in tqdm.tqdm(batches):\n",
    "    targets, contexts, labels = generate_training_data(batch, 5, 10, vocab_size, 42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3baeed84",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked = chunks(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1773d937",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain, islice\n",
    "\n",
    "def chunks(iterable, batchsize=10):\n",
    "    iterator = iter(iterable)\n",
    "    for first in iterator:\n",
    "        yield chain([first], islice(iterator, batchsize - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "d229114e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = chunks(corpus, batchsize=10000) # batches consists of generators, generating 15 sequences\n",
    "#for gen in batches:\n",
    "#    generate_training_data(gen, window_size, num_ns, vocab_size, 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c078257",
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = chunks(corpus, batchsize=15)\n",
    "t0 = time.time()\n",
    "generator_list = [gen for gen in batches]\n",
    "t1 = time.time()\n",
    "total = t1-t0\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604f7f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen0 = generator_list[1]\n",
    "generate_training_data(gen0, 5, 10, vocab_size, 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a586414e",
   "metadata": {},
   "outputs": [],
   "source": [
    "[gen for gen in list(batches)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "162251f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for gen in batches:\n",
    "    print(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b97ab40",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked = chunks(corpus, batchsize=15)\n",
    "for gen in tqdm.tqdm(chunked):\n",
    "    targets, contexts, labels = generate_training_data(gen, 5, 10, vocab_size, 42)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "53008316",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_data(sequences, window_size, num_ns, vocab_size, seed):\n",
    "    # Elements of each training example are appended to these lists.\n",
    "    targets, contexts, labels = [], [], []\n",
    "\n",
    "    # Build the sampling table for `vocab_size` tokens.\n",
    "    sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n",
    "\n",
    "    # Iterate over all sequences (sentences) in the dataset.\n",
    "    for sequence in tqdm.tqdm(sequences):\n",
    "\n",
    "    # Generate positive skip-gram pairs for a sequence (sentence).\n",
    "        positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "          sequence,\n",
    "          vocabulary_size=vocab_size,\n",
    "          sampling_table=sampling_table,\n",
    "          window_size=window_size,\n",
    "          negative_samples=0)\n",
    "\n",
    "    # Iterate over each positive skip-gram pair to produce training examples\n",
    "    # with a positive context word and negative samples.\n",
    "        for target_word, context_word in positive_skip_grams:\n",
    "            context_class = tf.expand_dims(\n",
    "                tf.constant([context_word], dtype=\"int64\"), 1)\n",
    "            negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "                true_classes=context_class,\n",
    "                num_true=1,\n",
    "                num_sampled=num_ns,\n",
    "                unique=True,\n",
    "                range_max=vocab_size,\n",
    "                seed=seed,\n",
    "                name=\"negative_sampling\")\n",
    "\n",
    "            # Build context and label vectors (for one target word)\n",
    "            negative_sampling_candidates = tf.expand_dims(\n",
    "            negative_sampling_candidates, 1)\n",
    "\n",
    "            context = tf.concat([context_class, negative_sampling_candidates], 0)\n",
    "            label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
    "\n",
    "            # Append each element from the training example to global lists.\n",
    "            targets.append(target_word)\n",
    "            contexts.append(context)\n",
    "            labels.append(label)\n",
    "\n",
    "    return targets, contexts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "05f62a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, corpus, window_size, num_ns, vocab_size, seed, batch_size=32, labels = None, shuffle=False ):\n",
    "        'Initialization'\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "        self.corpus = corpus\n",
    "        self.batches = self.init_batches(self.corpus, self.batch_size)\n",
    "        self.window_size = window_size\n",
    "        self.num_ns = num_ns\n",
    "        self.vocab_size = vocab_size\n",
    "        self.seed = seed\n",
    "        self.on_epoch_end()\n",
    "    \n",
    "    def init_batches(self, corpus, batchsize):\n",
    "        batched_corpus = chunks(corpus, batchsize)\n",
    "        batches = [gen for gen in batched_corpus]\n",
    "        return batches\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(generator_list)))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        batch_idx = self.indexes[index]\n",
    "        batch = self.batches[batch_idx] \n",
    "        targets, contexts, labels = self.__data_generation(batch)\n",
    "\n",
    "        return targets, contexts, labels\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.corpus))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "    \n",
    "    def __data_generation(self, batched_gen):\n",
    "        # Generate data\n",
    "        targets, contexts, labels = generate_training_data(batch, window_size, num_ns, vocab_size, seed = 42)\n",
    "        \n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f5e45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DataGenerator(corpus, window_size, num_ns, vocab_size, seed = 42, batch_size = 10000).__getitem__(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "9270e8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_generator = DataGenerator(generator_list, window_size, num_ns, vocab_size, seed = 42, batch_size = 10000)\n",
    "\n",
    "model = Word2Vec(vocab_size, embedding_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "1a7393db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_713/2856245566.py:7: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(generator=training_generator,\n",
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [213], line 7\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m strategy\u001b[38;5;241m.\u001b[39mscope():\n\u001b[1;32m      3\u001b[0m     model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      4\u001b[0m                  loss\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mCategoricalCrossentropy(from_logits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m      5\u001b[0m                  metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[1;32m      6\u001b[0m                 run_eagerly\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 7\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                        \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mworkers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m t1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     12\u001b[0m total \u001b[38;5;241m=\u001b[39m t1 \u001b[38;5;241m-\u001b[39m t0\n",
      "File \u001b[0;32m~/my-conda-envs/DomEnv/lib/python3.9/site-packages/keras/engine/training.py:2507\u001b[0m, in \u001b[0;36mModel.fit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2495\u001b[0m \u001b[38;5;124;03m\"\"\"Fits the model on data yielded batch-by-batch by a Python generator.\u001b[39;00m\n\u001b[1;32m   2496\u001b[0m \n\u001b[1;32m   2497\u001b[0m \u001b[38;5;124;03mDEPRECATED:\u001b[39;00m\n\u001b[1;32m   2498\u001b[0m \u001b[38;5;124;03m  `Model.fit` now supports generators, so there is no longer any need to\u001b[39;00m\n\u001b[1;32m   2499\u001b[0m \u001b[38;5;124;03m  use this endpoint.\u001b[39;00m\n\u001b[1;32m   2500\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2501\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   2502\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`Model.fit_generator` is deprecated and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2503\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwill be removed in a future version. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2504\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease use `Model.fit`, which supports generators.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2505\u001b[0m     stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m   2506\u001b[0m )\n\u001b[0;32m-> 2507\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2509\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2510\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2511\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2512\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2513\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2514\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2515\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2516\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2517\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2518\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2519\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2520\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2521\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2522\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/my-conda-envs/DomEnv/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[0;32mIn [211], line 31\u001b[0m, in \u001b[0;36mDataGenerator.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     29\u001b[0m batch_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindexes[index]\n\u001b[1;32m     30\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatches[batch_idx] \n\u001b[0;32m---> 31\u001b[0m targets, contexts, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__data_generation(batch)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m targets, contexts, labels\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "with strategy.scope():\n",
    "    model.compile(optimizer='adam',\n",
    "                 loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                 metrics=['accuracy'], \n",
    "                run_eagerly=True)\n",
    "    model.fit_generator(generator=training_generator,\n",
    "                        use_multiprocessing=True,\n",
    "                        workers = 6, \n",
    "                        epochs = 20)\n",
    "t1 = time.time()\n",
    "total = t1 - t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "9bc0bf56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23482it [01:14, 316.87it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [130], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m targets, contexts, labels \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_training_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mnum_ns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_ns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mSEED\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m targets \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(targets)\n\u001b[1;32m      8\u001b[0m contexts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(contexts)[:,:,\u001b[38;5;241m0\u001b[39m]\n",
      "Cell \u001b[0;32mIn [49], line 9\u001b[0m, in \u001b[0;36mgenerate_training_data\u001b[0;34m(sequences, window_size, num_ns, vocab_size, seed)\u001b[0m\n\u001b[1;32m      6\u001b[0m sampling_table \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mpreprocessing\u001b[38;5;241m.\u001b[39msequence\u001b[38;5;241m.\u001b[39mmake_sampling_table(vocab_size)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Iterate over all sequences (sentences) in the dataset.\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sequence \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(sequences):\n\u001b[1;32m     10\u001b[0m \n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Generate positive skip-gram pairs for a sequence (sentence).\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     positive_skip_grams, _ \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mpreprocessing\u001b[38;5;241m.\u001b[39msequence\u001b[38;5;241m.\u001b[39mskipgrams(\n\u001b[1;32m     13\u001b[0m       sequence,\n\u001b[1;32m     14\u001b[0m       vocabulary_size\u001b[38;5;241m=\u001b[39mvocab_size,\n\u001b[1;32m     15\u001b[0m       sampling_table\u001b[38;5;241m=\u001b[39msampling_table,\n\u001b[1;32m     16\u001b[0m       window_size\u001b[38;5;241m=\u001b[39mwindow_size,\n\u001b[1;32m     17\u001b[0m       negative_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Iterate over each positive skip-gram pair to produce training examples\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# with a positive context word and negative samples.\u001b[39;00m\n",
      "File \u001b[0;32m~/my-conda-envs/DomEnv/lib/python3.9/site-packages/tqdm/std.py:1205\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1203\u001b[0m dt \u001b[38;5;241m=\u001b[39m cur_t \u001b[38;5;241m-\u001b[39m last_print_t\n\u001b[1;32m   1204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dt \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m mininterval \u001b[38;5;129;01mand\u001b[39;00m cur_t \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m min_start_t:\n\u001b[0;32m-> 1205\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlast_print_n\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1206\u001b[0m     last_print_n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_print_n\n\u001b[1;32m   1207\u001b[0m     last_print_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_print_t\n",
      "File \u001b[0;32m~/my-conda-envs/DomEnv/lib/python3.9/site-packages/tqdm/std.py:1256\u001b[0m, in \u001b[0;36mtqdm.update\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m   1254\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ema_dn(dn)\n\u001b[1;32m   1255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ema_dt(dt)\n\u001b[0;32m-> 1256\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrefresh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlock_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlock_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1257\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdynamic_miniters:\n\u001b[1;32m   1258\u001b[0m     \u001b[38;5;66;03m# If no `miniters` was specified, adjust automatically to the\u001b[39;00m\n\u001b[1;32m   1259\u001b[0m     \u001b[38;5;66;03m# maximum iteration rate seen so far between two prints.\u001b[39;00m\n\u001b[1;32m   1260\u001b[0m     \u001b[38;5;66;03m# e.g.: After running `tqdm.update(5)`, subsequent\u001b[39;00m\n\u001b[1;32m   1261\u001b[0m     \u001b[38;5;66;03m# calls to `tqdm.update()` will only cause an update after\u001b[39;00m\n\u001b[1;32m   1262\u001b[0m     \u001b[38;5;66;03m# at least 5 more iterations.\u001b[39;00m\n\u001b[1;32m   1263\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxinterval \u001b[38;5;129;01mand\u001b[39;00m dt \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxinterval:\n",
      "File \u001b[0;32m~/my-conda-envs/DomEnv/lib/python3.9/site-packages/tqdm/std.py:1361\u001b[0m, in \u001b[0;36mtqdm.refresh\u001b[0;34m(self, nolock, lock_args)\u001b[0m\n\u001b[1;32m   1359\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1360\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39macquire()\n\u001b[0;32m-> 1361\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1362\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m nolock:\n\u001b[1;32m   1363\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/my-conda-envs/DomEnv/lib/python3.9/site-packages/tqdm/std.py:1509\u001b[0m, in \u001b[0;36mtqdm.display\u001b[0;34m(self, msg, pos)\u001b[0m\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pos:\n\u001b[1;32m   1508\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmoveto(pos)\n\u001b[0;32m-> 1509\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__str__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pos:\n\u001b[1;32m   1511\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmoveto(\u001b[38;5;241m-\u001b[39mpos)\n",
      "File \u001b[0;32m~/my-conda-envs/DomEnv/lib/python3.9/site-packages/tqdm/std.py:350\u001b[0m, in \u001b[0;36mtqdm.status_printer.<locals>.print_status\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprint_status\u001b[39m(s):\n\u001b[1;32m    349\u001b[0m     len_s \u001b[38;5;241m=\u001b[39m disp_len(s)\n\u001b[0;32m--> 350\u001b[0m     \u001b[43mfp_write\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\r\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlast_len\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlen_s\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    351\u001b[0m     last_len[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m len_s\n",
      "File \u001b[0;32m~/my-conda-envs/DomEnv/lib/python3.9/site-packages/tqdm/std.py:344\u001b[0m, in \u001b[0;36mtqdm.status_printer.<locals>.fp_write\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfp_write\u001b[39m(s):\n\u001b[1;32m    343\u001b[0m     fp\u001b[38;5;241m.\u001b[39mwrite(_unicode(s))\n\u001b[0;32m--> 344\u001b[0m     \u001b[43mfp_flush\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/my-conda-envs/DomEnv/lib/python3.9/site-packages/tqdm/utils.py:145\u001b[0m, in \u001b[0;36mDisableOnWriteError.disable_on_exception.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 145\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    147\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39merrno \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m5\u001b[39m:\n",
      "File \u001b[0;32m~/my-conda-envs/DomEnv/lib/python3.9/site-packages/ipykernel/iostream.py:480\u001b[0m, in \u001b[0;36mOutStream.flush\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpub_thread\u001b[38;5;241m.\u001b[39mschedule(evt\u001b[38;5;241m.\u001b[39mset)\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;66;03m# and give a timeout to avoid\u001b[39;00m\n\u001b[0;32m--> 480\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mevt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflush_timeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    481\u001b[0m         \u001b[38;5;66;03m# write directly to __stderr__ instead of warning because\u001b[39;00m\n\u001b[1;32m    482\u001b[0m         \u001b[38;5;66;03m# if this is happening sys.stderr may be the problem.\u001b[39;00m\n\u001b[1;32m    483\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIOStream.flush timed out\u001b[39m\u001b[38;5;124m\"\u001b[39m, file\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39m__stderr__)\n\u001b[1;32m    484\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/my-conda-envs/DomEnv/lib/python3.9/threading.py:581\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    579\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 581\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/my-conda-envs/DomEnv/lib/python3.9/threading.py:316\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 316\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    318\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "targets, contexts, labels = generate_training_data(corpus,\n",
    "                            window_size= window_size,\n",
    "                            num_ns=num_ns,\n",
    "                            vocab_size=vocab_size,\n",
    "                            seed = SEED)\n",
    " \n",
    "targets = np.array(targets)\n",
    "contexts = np.array(contexts)[:,:,0]\n",
    "labels = np.array(labels)\n",
    "\n",
    "print('\\n')\n",
    "print(f\"targets.shape: {targets.shape}\")\n",
    "print(f\"contexts.shape: {contexts.shape}\")\n",
    "print(f\"labels.shape: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf8ab99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5c26a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_ds = tf.data.TextLineDataset('corpus_text_shuffled.txt').filter(lambda x: tf.cast(tf.strings.length(x), bool))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7ff7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vectorize_layer = layers.TextVectorization(\n",
    "    max_tokens = vocab_size, \n",
    "    output_mode='int', \n",
    "    standardize = None,\n",
    "    split = 'whitespace',\n",
    "    output_sequence_length= 12221\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7265e576",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_layer.adapt(text_ds.batch(1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11eda3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_vocab = vectorize_layer.get_vocabulary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806e8db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vector_ds = text_ds.batch(1024).prefetch(AUTOTUNE).map(vectorize_layer).unbatch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e916c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = list(text_vector_ds.as_numpy_iterator())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5416bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0ba8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(sequences)) #39040 for output_sequence_length = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e8b6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets, contexts, labels = generate_training_data(\n",
    "    sequences=sequences,\n",
    "    window_size= window_size,\n",
    "    num_ns= num_ns,\n",
    "    vocab_size=vocab_size,\n",
    "    seed=SEED)\n",
    "\n",
    "targets = np.array(targets)\n",
    "contexts = np.array(contexts)[:,:,0]\n",
    "labels = np.array(labels)\n",
    "\n",
    "print('\\n')\n",
    "print(f\"targets.shape: {targets.shape}\")\n",
    "print(f\"contexts.shape: {contexts.shape}\")\n",
    "print(f\"labels.shape: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621048c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b91b76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1024\n",
    "BUFFER_SIZE = 100000\n",
    "dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1e5287",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8b27c7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(Word2Vec, self).__init__()\n",
    "        self.target_embedding = layers.Embedding(vocab_size,\n",
    "                                          embedding_dim,\n",
    "                                          input_length=1,\n",
    "                                          name=\"w2v_embedding\")\n",
    "        self.context_embedding = layers.Embedding(vocab_size,\n",
    "                                           embedding_dim,\n",
    "                                           input_length=num_ns+1)\n",
    "\n",
    "    def call(self, pair):\n",
    "        target, context = pair\n",
    "        # target: (batch, dummy?)  # The dummy axis doesn't exist in TF2.7+\n",
    "        # context: (batch, context)\n",
    "        if len(target.shape) == 2:\n",
    "            target = tf.squeeze(target, axis=1)\n",
    "        # target: (batch,)\n",
    "        word_emb = self.target_embedding(target)\n",
    "        # word_emb: (batch, embed)\n",
    "        context_emb = self.context_embedding(context)\n",
    "        # context_emb: (batch, context, embed)\n",
    "        dots = tf.einsum('be,bce->bc', word_emb, context_emb) \n",
    "        # dots: (batch, context)\n",
    "        return dots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbadc286",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(x_logit, y_true):\n",
    "      return tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=y_true)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfdfbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "model = Word2Vec(vocab_size, embedding_dim)\n",
    "model.compile(optimizer='adam',\n",
    "                 loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                 metrics=['accuracy'], \n",
    "                run_eagerly=True)\n",
    "model.fit(dataset, epochs=20)\n",
    "t1 = time.time()\n",
    "total = t1 - t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a71f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('training time: ', np.round(total/60, 2), ' mins')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b102b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0_g = time.time()\n",
    "model = Word2Vec(vocab_size, embedding_dim)\n",
    "model.compile(optimizer='adam',\n",
    "                 loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                 metrics=['accuracy'], \n",
    "                run_eagerly=True)\n",
    "with strategy.scope():\n",
    "    model.fit(dataset, epochs=20)\n",
    "t1_g = time.time()\n",
    "total_g = t1_g - t0_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128853c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75cbdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_g = model.get_layer('w2v_embedding').get_weights()[0]\n",
    "vocab_g = vectorize_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015a76eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_v_g = io.open('vectors_model_testing.tsv', 'w', encoding='utf-8')\n",
    "out_m_g = io.open('metadata_model_testing.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for index, word in enumerate(vocab):\n",
    "    if index == 0:\n",
    "        continue  # skip 0, it's padding.\n",
    "    vec = weights[index]\n",
    "    out_v_g.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "    out_m_g.write(word + \"\\n\")\n",
    "out_v_g.close()\n",
    "out_m_g.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (DomEnv)",
   "language": "python",
   "name": "domenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
